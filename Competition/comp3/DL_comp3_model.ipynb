{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import re\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[5], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_path ='./dictionary/'\n",
    "\n",
    "\n",
    "\n",
    "N_EPOCH_RNN = 250\n",
    "alpha = 0.8  # 0.4 0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5427 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 1\n",
      "Id to word mapping, for example: 1 -> flower\n",
      "Tokens: <PAD>: 5427; <RARE>: 5428\n"
     ]
    }
   ],
   "source": [
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the flower shown has yellow anther red pistil and bright red petals.\n",
      "['9', '1', '82', '5', '11', '70', '20', '31', '3', '29', '20', '2', '5427', '5427', '5427', '5427', '5427', '5427', '5427', '5427']\n"
     ]
    }
   ],
   "source": [
    "def sent2IdList(line, MAX_SEQ_LENGTH=20):\n",
    "    MAX_SEQ_LIMIT = MAX_SEQ_LENGTH\n",
    "    padding = 0\n",
    "    \n",
    "    # data preprocessing, remove all puntuation in the texts\n",
    "    prep_line = re.sub('[%s]' % re.escape(string.punctuation), ' ', line.rstrip())\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('  ', ' ')\n",
    "    prep_line = prep_line.replace('.', '')\n",
    "    tokens = prep_line.split(' ')\n",
    "    \n",
    "    tokens = [\n",
    "        tokens[i] for i in range(len(tokens))\n",
    "        if tokens[i] != ' ' and tokens[i] != ''\n",
    "    ]\n",
    "    \n",
    "    l = len(tokens)\n",
    "    padding = MAX_SEQ_LIMIT - l\n",
    "    \n",
    "    # make sure length of each text is equal to MAX_SEQ_LENGTH, and replace the less common word with <RARE> token\n",
    "    for i in range(padding):\n",
    "        tokens.append('<PAD>')\n",
    "    line = [\n",
    "        word2Id_dict[tokens[k]]\n",
    "        if tokens[k] in word2Id_dict else word2Id_dict['<RARE>']\n",
    "        for k in range(len(tokens))\n",
    "    ]\n",
    "\n",
    "    return line\n",
    "text = \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "print(text)\n",
    "print(sent2IdList(text))\n",
    "def idList2sent(idList, is_int=True):\n",
    "    if is_int:\n",
    "        return \" \".join([id2word_dict[str(i)] for i in idList if id2word_dict[str(i)] != \"<PAD>\" and id2word_dict[str(i)] != \"<RARE>\"])\n",
    "    else:\n",
    "        return \" \".join([id2word_dict[i] for i in idList if id2word_dict[i] != \"<PAD>\" and id2word_dict[i] != \"<RARE>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug_controller\n",
    "left_right_flip_p = 0.5\n",
    "noise_p = 0.3\n",
    "rotation_p = 0.4\n",
    "crop_p = 0.3\n",
    "contrast_p = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def random_flip(image):\n",
    "    right_left_outcome = tf.random.uniform([1], 0, 1)\n",
    "    if right_left_outcome<left_right_flip_p:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "    return image\n",
    "\n",
    "@tf.function\n",
    "def random_rotation(image):\n",
    "    prob = tf.random.uniform([1],0,1)\n",
    "    if prob < (rotation_p/2):\n",
    "        image = tf.image.rot90(image, k = 1 )\n",
    "    elif (prob >= (rotation_p/2)) and (prob < rotation_p):\n",
    "        image = tf.image.rot90(image, k = 3 )   \n",
    "    return image\n",
    "\n",
    "@tf.function\n",
    "def random_crop(image):\n",
    "    prob = tf.random.uniform([1],0,1)\n",
    "    if prob < crop_p:\n",
    "        image = tf.image.resize(image, size=[IMAGE_HEIGHT + 5, IMAGE_WIDTH + 5])\n",
    "        image = tf.image.random_crop(image, size=[IMAGE_HEIGHT, IMAGE_WIDTH, 3])\n",
    "    else:\n",
    "        image = tf.image.resize(image, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "        \n",
    "    return image\n",
    "@tf.function\n",
    "def adj_contrast(image):\n",
    "    prob = tf.random.uniform([1], 0, 1)\n",
    "    if prob < contrast_p:\n",
    "        image = tf.image.random_contrast(image, 0.9, 2)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "\n",
    "def training_data_generator(caption, image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img.set_shape([None, None, 3])\n",
    "    short_side = tf.minimum(tf.shape(img)[0], tf.shape(img)[1])\n",
    "    img = tf.image.resize_with_crop_or_pad(img, short_side, short_side)\n",
    "    img = tf.image.resize(img, size=[64 * 76 // 64, 64 * 76 // 64])\n",
    "    img = tf.image.random_crop(img, [64,64,3])\n",
    "    img = img*2-1.\n",
    "    noise = tf.random.normal(img.shape,stddev=0.02)\n",
    "    img = tf.math.add(img,noise)\n",
    "    img = tf.image.random_flip_up_down(img)\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    caption = tf.cast(caption, tf.int32)\n",
    "    return img, caption\n",
    "\n",
    "def training_data_generator_only_image(caption, image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img.set_shape([None, None, 3])\n",
    "    short_side = tf.minimum(tf.shape(img)[0], tf.shape(img)[1])\n",
    "    img = tf.image.resize_with_crop_or_pad(img, short_side, short_side)\n",
    "    img = tf.image.resize(img, size=[64 * 76 // 64, 64 * 76 // 64])\n",
    "    img = tf.image.random_crop(img, [64,64,3])\n",
    "    img = img*2-1.\n",
    "    noise = tf.random.normal(img.shape,stddev=0.02)\n",
    "    img = tf.math.add(img,noise)\n",
    "    img = tf.image.random_flip_up_down(img)\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def training_data_generator_only_caption(caption, image_path):\n",
    "    caption = tf.cast(caption, tf.int32)\n",
    "    return caption\n",
    "\n",
    "def training_data_generator_rnn(caption, image_path):\n",
    "    # load in the image according to image path\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img.set_shape([None, None, 3])\n",
    "    img = tf.image.resize(img, size=[128, 128])\n",
    "    img.set_shape([128, 128, IMAGE_CHANNEL])\n",
    "    \n",
    "    img = tf.image.resize_with_crop_or_pad(img,128 + 10, 128 + 10)\n",
    "    img = tf.image.random_crop(img, [128,128, IMAGE_CHANNEL])    \n",
    "    noise = tf.random.normal(img.shape,stddev=0.01)\n",
    "    img = tf.math.add(img,noise)\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_flip_up_down(img)\n",
    "    caption = tf.cast(caption, tf.int32)\n",
    "    return img, caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_generator():\n",
    "    \n",
    "    def __init__(self, filenames, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        df = pd.read_pickle(filenames)\n",
    "        captions = df['Captions'].values\n",
    "        self.caption = []\n",
    "        for i in range(len(captions)):\n",
    "            self.caption.append(random.choice(captions[i]))\n",
    "        self.caption = np.asarray(self.caption)\n",
    "        self.caption = self.caption.astype(np.int)\n",
    "        self.image_path = df['ImagePath'].values\n",
    "        assert self.caption.shape[0] == self.image_path.shape[0]\n",
    "        \n",
    "    def generate(self, generate_func):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((self.caption,self.image_path))\n",
    "        dataset = dataset.map(generate_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        dataset = dataset.shuffle(len(self.caption)).batch(self.batch_size, drop_remainder=True)\n",
    "        dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "        \n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = {\n",
    "    'MAX_SEQ_LENGTH': 20,                     # maximum sequence length\n",
    "    'EMBED_DIM': 256,                         # word embedding dimension\n",
    "    'VOCAB_SIZE': len(word2Id_dict),          # size of dictionary of captions\n",
    "    'TEXT_EMBEDDING_REDUCTION':128,\n",
    "    \"T_DIM\": 64,\n",
    "    'Z_DIM': 100,                             # random noise z dimension\n",
    "    'IMAGE_SIZE': [64, 64, 3],                # render image size\n",
    "    'BATCH_SIZE': 64,\n",
    "    'BATCH_SIZE_RNN': 64,\n",
    "    'LR': 2e-4,\n",
    "    'RNN-CNN-LR':2e-4,\n",
    "    'LR_DECAY': 0.5,\n",
    "    'BETA_1': 0.5,\n",
    "    'N_EPOCH': 2000,\n",
    "    'N_EPOCH_RNN': 250, # 50\n",
    "    'N_SAMPLE': num_training_sample,          # size of training data\n",
    "    'PRINT_FREQ': 1                        # printing frequency of loss                        \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_generator = dataset_generator(data_path + '/text2ImgData.pkl', par['BATCH_SIZE_RNN'])\n",
    "dataset = ds_generator.generate(training_data_generator)\n",
    "dataset_wrong_image = ds_generator.generate(training_data_generator_only_image)\n",
    "dataset_wrong_caption = ds_generator.generate(training_data_generator_only_caption)\n",
    "dataset_rnn = ds_generator.generate(training_data_generator_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(tf.keras.Model):\n",
    "    def __init__(self, par):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        \n",
    "        self.par = par\n",
    "        self.batch_size = self.par['BATCH_SIZE_RNN']\n",
    "        self.hidden_size = int(self.par['EMBED_DIM'])\n",
    "\n",
    "        self.embedding = layers.Embedding(self.par['VOCAB_SIZE'], self.par['EMBED_DIM'],\n",
    "                                        embeddings_initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        \n",
    "        self.lstm = layers.LSTM(self.hidden_size,\n",
    "                                return_sequences=True,\n",
    "                                return_state=True,\n",
    "                                recurrent_initializer='orthogonal')\n",
    "        \n",
    "    \n",
    "    def call(self, text, hidden):\n",
    "        text = self.embedding(text)\n",
    "        output= self.lstm(text, initial_state = hidden)\n",
    "        return output[0][:,-1,:]\n",
    "    def initialize_hidden_state(self):\n",
    "        #return [tf.zeros((self.par['BATCH_SIZE_RNN'], self.hidden_size)) for i in range(2)]\n",
    "        return [tf.zeros((self.par['BATCH_SIZE_RNN'], self.hidden_size)) for i in range(2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn_encoder(tf.keras.Model):\n",
    "    def __init__(self, par):\n",
    "        super(cnn_encoder, self).__init__()\n",
    "        self.par = par\n",
    "        self.df_dim = 64\n",
    "        #h0\n",
    "        self.c01 = tf.keras.layers.Conv2D(\n",
    "            filters = self.df_dim,\n",
    "            kernel_size = 4,\n",
    "            strides = (2, 2),\n",
    "            padding = \"SAME\",\n",
    "            input_shape = (128,128,3)\n",
    "        )\n",
    "        self.r01 = tf.keras.layers.LeakyReLU(0.2)\n",
    "        #h1\n",
    "        self.c11 = tf.keras.layers.Conv2D(\n",
    "            filters = self.df_dim*2,\n",
    "            kernel_size = 4,\n",
    "            strides = (2, 2),\n",
    "            padding = \"SAME\"        \n",
    "        )\n",
    "        self.b11 = tf.keras.layers.BatchNormalization()\n",
    "        self.r11 = tf.keras.layers.LeakyReLU(0.2)\n",
    "            \n",
    "        #h2\n",
    "        self.c21 = tf.keras.layers.Conv2D(\n",
    "            filters = self.df_dim*4,\n",
    "            kernel_size = 4,\n",
    "            strides = (2, 2),\n",
    "            padding = \"SAME\"        \n",
    "        )\n",
    "        self.b21 = tf.keras.layers.BatchNormalization()\n",
    "        self.r21 = tf.keras.layers.LeakyReLU(0.2)\n",
    "        #h3\n",
    "        self.c31 = tf.keras.layers.Conv2D(\n",
    "            filters = self.df_dim*8,\n",
    "            kernel_size = 4,\n",
    "            strides = (2, 2),\n",
    "            padding = \"SAME\"        \n",
    "        )\n",
    "        self.b31 = tf.keras.layers.BatchNormalization()\n",
    "        self.r31 = tf.keras.layers.LeakyReLU(0.2)\n",
    "        \n",
    "        #h4\n",
    "        self.c41 = tf.keras.layers.Conv2D(\n",
    "            filters = self.df_dim*8,\n",
    "            kernel_size = 4,\n",
    "            strides = (2, 2),\n",
    "            padding = \"SAME\"        \n",
    "        )\n",
    "        self.b41 = tf.keras.layers.BatchNormalization()\n",
    "        self.r41 = tf.keras.layers.LeakyReLU(0.2)\n",
    "        \n",
    "        #final\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.d = tf.keras.layers.Dense(256)\n",
    "    def call(self, inputs):\n",
    "        h0 = self.c01(inputs)\n",
    "        h0 = self.r01(h0)\n",
    "        \n",
    "        #h1\n",
    "        h1 = self.c11(h0)\n",
    "        h1 = self.b11(h1)\n",
    "        h1 = self.r11(h1)\n",
    "        \n",
    "        #h2\n",
    "        h2 = self.c21(h1)\n",
    "        h2 = self.b21(h2)\n",
    "        h2 = self.r21(h2)\n",
    "        #h3\n",
    "        h3 = self.c31(h2)\n",
    "        h3 = self.b31(h3)\n",
    "        h3 = self.r31(h3)       \n",
    "        #h4\n",
    "        #h4 = self.c41(h3)\n",
    "        #h4 = self.b41(h4)\n",
    "        #h4 = self.r41(h4)  \n",
    "        #final\n",
    "        final = self.flatten(h3)\n",
    "        final = self.d(final)\n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    cost = tf.reduce_sum(tf.multiply(v1, v2), 1) / (tf.sqrt(tf.reduce_sum(tf.multiply(v1, v1), 1)) * tf.sqrt(tf.reduce_sum(tf.multiply(v2, v2), 1)))\n",
    "    return cost\n",
    "def rnn_loss(real_cnn, text_embed, wrong_cnn, wrong_text_embed):\n",
    "    rnn_loss = rnn_loss = tf.reduce_mean(tf.maximum(0., alpha - cosine_similarity(real_cnn, text_embed) + cosine_similarity(real_cnn, wrong_text_embed))) + \\\n",
    "                tf.reduce_mean(tf.maximum(0., alpha - cosine_similarity(real_cnn, text_embed) + cosine_similarity(wrong_cnn, text_embed)))\n",
    "    return rnn_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = TextEncoder(par)\n",
    "cnn_encoder = cnn_encoder(par)\n",
    "rnn_optimizer = tf.keras.optimizers.Adam(par['RNN-CNN-LR'], clipnorm=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = './tune_ckpt/alpha_'+str(int(10*alpha))\n",
    "os.makedirs(main_path+'/word-cnn-rnn/', exist_ok =True)\n",
    "checkpoint_path = main_path+'/word-cnn-rnn/'# './tune_ckpt/word-cnn-rnn/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume training from epoch 250\n"
     ]
    }
   ],
   "source": [
    "ckpt = tf.train.Checkpoint(rnn_optimizer=rnn_optimizer,\n",
    "                        text_encoder=text_encoder)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "\n",
    "print(f'Resume training from epoch {start_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_rnn(real_image, real_caption, hidden, wrong_image, wrong_caption, wrong_hidden):\n",
    "    with tf.GradientTape() as rnn_tape:\n",
    "        x = cnn_encoder(real_image)\n",
    "        v = text_encoder(real_caption, hidden)\n",
    "        x_w = cnn_encoder(wrong_image)\n",
    "        v_w = text_encoder(wrong_caption, wrong_hidden)\n",
    "        r_loss = rnn_loss(x, v, x_w, v_w)\n",
    "        \n",
    "    grad_r = rnn_tape.gradient(r_loss, text_encoder.trainable_variables + cnn_encoder.trainable_variables)\n",
    "    rnn_optimizer.apply_gradients(zip(grad_r, text_encoder.trainable_variables + cnn_encoder.trainable_variables))\n",
    "    return r_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = int(par['N_SAMPLE']/par['BATCH_SIZE_RNN'])\n",
    "hidden = text_encoder.initialize_hidden_state()\n",
    "mis_hidden = text_encoder.initialize_hidden_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch,par['N_EPOCH_RNN']):\n",
    "    r_total_loss = 0\n",
    "    start = time.time()\n",
    "    ################################################################\n",
    "    ds_generator = dataset_generator(data_path + '/text2ImgData.pkl', par['BATCH_SIZE_RNN'])\n",
    "    # dataset = ds_generator.generate(training_data_generator)\n",
    "    # dataset_wrong_image = ds_generator.generate(training_data_generator_only_image)\n",
    "    # dataset_wrong_caption = ds_generator.generate(training_data_generator_only_caption)\n",
    "    dataset_rnn = ds_generator.generate(training_data_generator_rnn)\n",
    "    ############################################################\n",
    "    for image, caption in dataset_rnn:\n",
    "        mismatch_cap = caption\n",
    "        break\n",
    "    for image, caption in dataset_rnn:\n",
    "        mismatch_img = image\n",
    "        break\n",
    "    for image, caption in dataset_rnn:\n",
    "        r_loss = train_step_rnn(image, caption, hidden,mismatch_img,mismatch_cap,mis_hidden)\n",
    "        r_total_loss += r_loss\n",
    "        k = np.random.uniform()\n",
    "        if k>=0.5:\n",
    "            mismatch_cap = caption\n",
    "        if k<0.5:\n",
    "            mismatch_img = image\n",
    "    time_tuple = time.localtime()\n",
    "    time_string = time.strftime(\"%m/%d/%Y, %H:%M:%S\", time_tuple)\n",
    "\n",
    "\n",
    "    print(\"Epoch {}, rnn_loss: {:.4f}\".format(epoch+1,\n",
    "                                                r_total_loss/steps_per_epoch))\n",
    "    print('Time for epoch {} is {:.4f} sec'.format(epoch+1, time.time()-start))\n",
    "    \n",
    "    # save the model\n",
    "    if epoch  == 50-1 or epoch  == 150-1 or epoch  == 250-1  : # if epoch % 10 == 9:\n",
    "        ckpt_manager.save(checkpoint_number=(epoch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f6210155f10>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "par['N_EPOCH_RNN'] = N_EPOCH_RNN\n",
    "# ckpt.restore(main_path+'/word-cnn-rnn/ckpt-'+str(1))\n",
    "ckpt.restore(main_path+'/word-cnn-rnn/ckpt-'+str(N_EPOCH_RNN))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class affine(tf.keras.Model):\n",
    "    def __init__(self, num_features):\n",
    "        super(affine, self).__init__()\n",
    "        self.fc_gamma = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(256),\n",
    "            tf.keras.layers.ReLU(),          \n",
    "            tf.keras.layers.Dense(num_features)\n",
    "            ])\n",
    "        self.fc_beta = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(256),\n",
    "            tf.keras.layers.ReLU(),          \n",
    "            tf.keras.layers.Dense(num_features)\n",
    "            ])\n",
    "    def call(self,picture,text):\n",
    "        weight = self.fc_gamma(text)\n",
    "        bias = self.fc_beta(text)        \n",
    "        weight = tf.expand_dims(tf.expand_dims(weight,1),1)\n",
    "        weight = tf.tile(weight,[1,picture.shape[1],picture.shape[2],1])\n",
    "        bias = tf.expand_dims(tf.expand_dims(bias,1),1)\n",
    "        bias = tf.tile(bias,[1,picture.shape[1],picture.shape[2],1])\n",
    "        return weight * picture + bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G_Block(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(G_Block, self).__init__()\n",
    "\n",
    "        self.learnable_sc = in_ch != out_ch \n",
    "        self.c1 = tf.keras.layers.Conv2D(out_ch,3,1,padding = 'SAME')\n",
    "        self.c2 = tf.keras.layers.Conv2D(out_ch,3,1,padding = 'SAME')\n",
    "\n",
    "        self.affine0 = affine(in_ch)\n",
    "        self.affine1 = affine(in_ch)\n",
    "        self.affine2 = affine(out_ch)\n",
    "        self.affine3 = affine(out_ch)\n",
    "        \n",
    "        self.gamma = tf.Variable(tf.zeros(1),trainable=True)\n",
    "        if self.learnable_sc:\n",
    "            self.c_sc = tf.keras.layers.Conv2D(out_ch, 1, 1, padding = 'valid')\n",
    "    def shortcut(self, picture):\n",
    "        if self.learnable_sc:\n",
    "            picture = self.c_sc(picture)\n",
    "        return picture\n",
    "    def residual(self, picture, text):\n",
    "        h = self.affine0(picture, text)\n",
    "        h = tf.nn.leaky_relu(h)\n",
    "        h = self.affine1(h, text)\n",
    "        h = tf.nn.leaky_relu(h)\n",
    "        h = self.c1(h)\n",
    "        h = self.affine2(h, text)\n",
    "        h = tf.nn.leaky_relu(h)\n",
    "        h = self.affine3(h, text)\n",
    "        h = tf.nn.leaky_relu(h)\n",
    "        return self.c2(h)\n",
    "    \n",
    "    def call(self, picture, text):\n",
    "        return self.shortcut(picture) + self.gamma * self.residual(picture, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetG(tf.keras.Model):\n",
    "    def __init__(self,par):\n",
    "        super(NetG, self).__init__()\n",
    "        ngf = par['T_DIM']\n",
    "        ngf = par['Z_DIM']\n",
    "        self.ngf = ngf\n",
    "        self.nz = 100\n",
    "        # layer1 input 100x1x1 random noise , output size (ngf*8)x4x4\n",
    "        self.fc = tf.keras.layers.Dense(ngf*8*4*4)\n",
    "        self.block0 = G_Block(ngf * 8, ngf * 8)#4x4\n",
    "        self.block1 = G_Block(ngf * 8, ngf * 8)#4x4\n",
    "        self.block2 = G_Block(ngf * 8, ngf * 4)#8x8\n",
    "        self.block3 = G_Block(ngf * 4, ngf * 2)#16x16\n",
    "        self.block4 = G_Block(ngf * 2, ngf * 1)#32x32\n",
    "        #self.block5 = G_Block(ngf * 2, ngf * 1)#64x64\n",
    "\n",
    "        self.conv_img = tf.keras.Sequential(\n",
    "            [tf.keras.layers.LeakyReLU(0.2),\n",
    "            tf.keras.layers.Conv2D(3, 3, 1, padding = 'SAME',activation='tanh')]\n",
    "        )\n",
    "    def call(self, text,noise):\n",
    "        out = self.fc(noise)\n",
    "        out = tf.reshape(out,[64,4,4,8*self.ngf])\n",
    "        out = self.block0(out,text) \n",
    "        #4*4\n",
    "        out = tf.image.resize(out,[out.shape[1]*2,out.shape[2]*2])\n",
    "        out = self.block1(out,text)\n",
    "        #8*8\n",
    "        out = tf.image.resize(out,[out.shape[1]*2,out.shape[2]*2])\n",
    "        out = self.block2(out,text)\n",
    "        #16*16\n",
    "        out = tf.image.resize(out,[out.shape[1]*2,out.shape[2]*2])\n",
    "        out = self.block3(out,text)\n",
    "        #32*32\n",
    "        out = tf.image.resize(out,[out.shape[1]*2,out.shape[2]*2])\n",
    "        out = self.block4(out,text)\n",
    "        #64*64\n",
    "        out = self.conv_img(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class resD(tf.keras.Model):\n",
    "    def __init__(self, fin, fout, downsample=True):\n",
    "        super().__init__()\n",
    "        self.downsample = downsample\n",
    "        self.learned_shortcut = (fin != fout)\n",
    "        self.conv_r = tf.keras.Sequential(\n",
    "            [tf.keras.layers.Conv2D(fout, 4, 2, padding = \"SAME\", use_bias=False),\n",
    "            tf.keras.layers.LeakyReLU(0.2),\n",
    "\n",
    "            tf.keras.layers.Conv2D(fout, 3, 1, padding = 'SAME', use_bias=False),\n",
    "            tf.keras.layers.LeakyReLU(0.2),]\n",
    "        )\n",
    "\n",
    "        self.conv_s = tf.keras.layers.Conv2D(fout, 1, 1, padding = 'valid')\n",
    "        self.gamma = tf.Variable(tf.zeros(1))\n",
    "    def shortcut(self, x):\n",
    "        if self.learned_shortcut:\n",
    "            x = self.conv_s(x)\n",
    "        if self.downsample:\n",
    "            return tf.nn.avg_pool2d(x, 2,2,padding = 'VALID')\n",
    "        return x\n",
    "    def residual(self, x):\n",
    "        return self.conv_r(x)\n",
    "    def call(self, x, c=None):\n",
    "        return self.shortcut(x)+self.gamma*self.residual(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetD(tf.keras.Model):\n",
    "    def __init__(self, ndf):\n",
    "        super(NetD, self).__init__()\n",
    "\n",
    "        self.conv_img = tf.keras.layers.Conv2D(ndf, 3, 1, padding='SAME',use_bias=False)#128\n",
    "        self.block0 = resD(ndf,ndf * 2)     #64\n",
    "        self.block1 = resD(ndf*2,ndf * 4)   #32\n",
    "        self.block2 = resD(ndf*4,ndf * 8)   #16\n",
    "        self.block3 = resD(ndf*8,ndf * 16)  #8\n",
    "        \n",
    "\n",
    "        self.joint_conv = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(ndf * 2, 3, 1, padding='SAME', use_bias=False),\n",
    "            tf.keras.layers.LeakyReLU(0.2),\n",
    "            tf.keras.layers.Conv2D(1, 4, 1, use_bias=False)]\n",
    "        )\n",
    "\n",
    "    def call(self,picture,text):\n",
    "\n",
    "        out = self.conv_img(picture)\n",
    "        out = self.block0(out)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "\n",
    "        text = tf.expand_dims(tf.expand_dims(text,1),1)\n",
    "        text = tf.tile(text,[1,4,4,1])\n",
    "        pc = tf.concat([out,text],axis = 3)\n",
    "        out = self.joint_conv(pc)\n",
    "        return tf.reshape(out,[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_wgp = NetG(par)\n",
    "discriminator_wgp = NetD(64)\n",
    "generator_wgp_optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001)\n",
    "discriminator_wgp_optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    # return cross_entropy(tf.ones_like(fake_output), fake_output)+1*KL_loss(mean,tf.math.log(std))\n",
    "    return -tf.reduce_mean(fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def train_step(real_image, real_caption,real_hidden, wrong_caption,wrong_hidden):\n",
    "    noise = tf.random.normal(shape=[par['BATCH_SIZE'], par['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "    real_text_embed  = text_encoder(real_caption, real_hidden,training = False)\n",
    "    wrong_text_embed  = text_encoder(wrong_caption, wrong_hidden,training = False)\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        with tf.GradientTape() as zero_tape1,tf.GradientTape() as zero_tape2:\n",
    "            zero_tape1.watch(real_image)\n",
    "            zero_tape2.watch(real_text_embed)\n",
    "            real_image_real_text = discriminator_wgp(real_image, real_text_embed,training = True)\n",
    "        g1 = zero_tape1.gradient(real_image_real_text,real_image)\n",
    "        g2 = zero_tape2.gradient(real_image_real_text,real_text_embed)        \n",
    "        g1 = tf.sqrt(tf.reduce_sum(g1**2,[1,2,3]))\n",
    "        g2 = tf.sqrt(tf.reduce_sum(g2**2,[1]))\n",
    "        g = tf.reduce_mean((g1+g2)**6)\n",
    "        fake_image = generator_wgp(real_text_embed, noise)\n",
    "        fake_image_real_text = discriminator_wgp(fake_image, real_text_embed,training = True)\n",
    "        real_image_fake_text = discriminator_wgp(real_image, wrong_text_embed,training = True)\n",
    "        real_loss = tf.reduce_mean(tf.nn.relu(1-real_image_real_text))\n",
    "        fake_loss = tf.reduce_mean(tf.nn.relu(1+fake_image_real_text))\n",
    "        mismatch_loss = tf.reduce_mean(tf.nn.relu(1+real_image_fake_text))\n",
    "        d_loss = real_loss+(fake_loss+mismatch_loss)/2+2*g\n",
    "    grad_d = disc_tape.gradient(d_loss, discriminator_wgp.trainable_variables)\n",
    "    discriminator_wgp_optimizer.apply_gradients(zip(grad_d, discriminator_wgp.trainable_variables))\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        fake_image = generator_wgp(real_text_embed, noise,training = True)\n",
    "        fake_image_real_text = discriminator_wgp(fake_image, real_text_embed,training = True)\n",
    "        g_loss = generator_loss(fake_image_real_text)\n",
    "    grad_g = gen_tape.gradient(g_loss, generator_wgp.trainable_variables)\n",
    "    generator_wgp_optimizer.apply_gradients(zip(grad_g, generator_wgp.trainable_variables))\n",
    "    return g_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path_gen = main_path +'/gen_fusion_N_EPOCH_RNN_'+ str(1)\n",
    "checkpoint_path_gen = main_path +'/gen_fusion_N_EPOCH_RNN_'+ str(N_EPOCH_RNN)\n",
    "os.makedirs(checkpoint_path_gen, exist_ok =True)\n",
    "checkpoint_name_gen = 'implement_gen_fusion'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume training from epoch 1200\n"
     ]
    }
   ],
   "source": [
    "ckpt_4 = tf.train.Checkpoint(generator_wgp_optimizer=generator_wgp_optimizer,\n",
    "                           discriminator_wgp_optimizer=discriminator_wgp_optimizer,\n",
    "                           generator_wgp=generator_wgp,\n",
    "                           discriminator_wgp=discriminator_wgp)\n",
    "\n",
    "manager_2 = tf.train.CheckpointManager(ckpt_4, checkpoint_path_gen, max_to_keep=40,\n",
    "                                     checkpoint_name=checkpoint_name_gen)\n",
    "start_epoch = 0\n",
    "if manager_2.latest_checkpoint:\n",
    "    start_epoch = int(manager_2.latest_checkpoint.split('-')[-1])\n",
    "    ckpt_4.restore(manager_2.latest_checkpoint)\n",
    "print(f'Resume training from epoch {start_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_generator(caption, batch_size):\n",
    "    caption = np.asarray(caption)\n",
    "    caption = caption.astype(np.int)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(caption)\n",
    "    dataset = dataset.batch(batch_size) \n",
    "    return dataset\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "    return img\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    # getting the pixel values between [0, 1] to save it\n",
    "    return plt.imsave(path, merge(images, size))\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(images, size, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni = 8\n",
    "sample_size = 64\n",
    "sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, par['Z_DIM'])).astype(np.float32)\n",
    "sample_sentence = [\"the flower shown has yellow anther red pistil and bright red petals.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has petals that are yellow, white and purple and has dark lines\"] * int(sample_size/ni) + \\\n",
    "                  [\"the petals on this flower are white with a yellow center\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has a lot of small round pink petals.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower is orange in color, and has petals that are ruffled and rounded.\"] * int(sample_size/ni) + \\\n",
    "                  [\"the flower has yellow petals and the center of it is brown.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has petals that are blue and white.\"] * int(sample_size/ni) +\\\n",
    "                  [\"these white flowers have petals th at start off white in color and end in a white towards the tips.\"] * int(sample_size/ni)\n",
    "\n",
    "for i, sent in enumerate(sample_sentence):\n",
    "    sample_sentence[i] = sent2IdList(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence = sample_generator(sample_sentence, par['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(caption, noise, hidden):\n",
    "    text_embed  = text_encoder(caption, hidden,training = False)\n",
    "    fake_image= generator_wgp(text_embed, noise,training=False)\n",
    "    return (fake_image+1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_DIR = main_path + '/implement/N_EPOCH_RNN_'+ str(N_EPOCH_RNN)\n",
    "os.makedirs(SAMPLE_DIR, exist_ok =True)\n",
    "\n",
    "steps_per_epoch = int(par['N_SAMPLE']/par['BATCH_SIZE'])\n",
    "hidden = text_encoder.initialize_hidden_state()\n",
    "wrong_hidden = text_encoder.initialize_hidden_state()\n",
    "t_hidden = text_encoder.initialize_hidden_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch,2000):\n",
    "    g_total_loss = 0\n",
    "    d_total_loss = 0\n",
    "    start = time.time()\n",
    "    ##########################################################\n",
    "    ds_generator = dataset_generator(data_path + '/text2ImgData.pkl', par['BATCH_SIZE_RNN'])\n",
    "    dataset = ds_generator.generate(training_data_generator)\n",
    "    # dataset_wrong_image = ds_generator.generate(training_data_generator_only_image)\n",
    "    dataset_wrong_caption = ds_generator.generate(training_data_generator_only_caption)\n",
    "    ########################################################## \n",
    "    for idx, ((image_real, caption_real),caption_wrong) in enumerate(zip(dataset,dataset_wrong_caption)):\n",
    "\n",
    "        g_loss, d_loss = train_step(image_real, caption_real,hidden,caption_wrong,wrong_hidden)\n",
    "        g_total_loss += g_loss\n",
    "        d_total_loss += d_loss\n",
    "\n",
    "    print(\"Epoch {}, gen_loss: {:.4f}, disc_loss: {:.4f}\".format(epoch+1,\n",
    "                                                                g_total_loss/steps_per_epoch,\n",
    "                                                                d_total_loss/steps_per_epoch))        \n",
    "    print('Time for epoch {} is {:.4f} sec'.format(epoch+1, time.time()-start))\n",
    "\n",
    "    # save the model\n",
    "    if epoch % 50 == 49: # if epoch  == 600-1 or epoch  == 1100-1 or epoch  == 1400-1  : #\n",
    "        manager_2.save(checkpoint_number=(epoch + 1))\n",
    "\n",
    "    # visualization\n",
    "    if (epoch + 1) % par['PRINT_FREQ'] == 0:\n",
    "        for caption in sample_sentence:\n",
    "            fake_image = test_step(caption, sample_seed, t_hidden)\n",
    "        save_images(fake_image, [ni, ni],  SAMPLE_DIR + 'train_{:02d}.jpg'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_data_generator(caption, index):\n",
    "    caption = tf.cast(caption, tf.float32)\n",
    "    return caption, index\n",
    "\n",
    "def testing_dataset_generator(batch_size, data_generator):\n",
    "    data = pd.read_pickle('./dataset/testData.pkl')\n",
    "    captions = data['Captions'].values\n",
    "    caption = []\n",
    "    for i in range(len(captions)):\n",
    "        caption.append(captions[i])\n",
    "    caption = np.asarray(caption)\n",
    "    caption = caption.astype(np.int)\n",
    "    index = data['ID'].values\n",
    "    index = np.asarray(index)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, index))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "testing_dataset = testing_dataset_generator(par['BATCH_SIZE'], testing_data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "captions = data['Captions'].values\n",
    "\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / par['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset):\n",
    "    hidden = text_encoder.initialize_hidden_state()\n",
    "    sample_size = par['BATCH_SIZE']\n",
    "    sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, par['Z_DIM'])).astype(np.float32)\n",
    "    \n",
    "    step = 0\n",
    "    start = time.time()\n",
    "    for captions, idx in dataset:\n",
    "        if step > EPOCH_TEST:\n",
    "            break\n",
    "        \n",
    "        fake_image = test_step(captions, sample_seed, hidden)\n",
    "        step += 1\n",
    "        for i in range(par['BATCH_SIZE']):\n",
    "            plt.imsave('/NA/rabbit2/JBR/Dominic_test/' + 'inference_{:04d}.jpg'.format(idx[i]), fake_image[i].numpy())\n",
    "            \n",
    "    print('Time for inference is {:.4f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference is 10.2367 sec\n"
     ]
    }
   ],
   "source": [
    "inference(testing_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visible devices cannot be modified after being initialized\n",
      "--------------Evaluation Success-----------------\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/NA/rabbit2/JBR/testing')\n",
    "%run /NA/rabbit2/JBR/testing/inception_score.py /NA/rabbit2/JBR/Dominic_test /NA/rabbit2/JBR/dominic_score_demoV6.csv 39\n",
    "#os.chdir('/home/johnny/Desktop/competition3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
