{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 伯伯會社\n",
    "## 組員\n",
    "## 江伯耕 108024517\n",
    "## 蔣嘉霖 108024514\n",
    "## 周秉儒 107024703\n",
    "## 陳炘昱 107024701"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "%matplotlib inline\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from scipy.sparse import csr_matrix\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "import datetime as datetime\n",
    "from datetime import date\n",
    "import pytz as pytz\n",
    "from gensim import corpora,models\n",
    "import pickle\n",
    "import holidays\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import wordnet\n",
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、建構feature "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 首先去除html tags後，將文章的內容做tokenization處理，並利用lemmatization做詞性的還原以及刪除Stop-words後抓取下列的feature： \n",
    "1. 標題(Bag-Of-Words) \n",
    "2. 作者姓名 (one hot encoding) \n",
    "3. 時間 \n",
    "4. 文章channel (one hot encoding) \n",
    "5. 文章底部標籤 (Bag-Of-Words) \n",
    "6. 文章內容 (Latent Dirichlet Allocation) \n",
    "7. figure caption內容 (Bag-Of-Words) \n",
    "8. Seealso 數量 \n",
    "9. Insragram數量 \n",
    "10. Twitter數量\n",
    "11. mashable連結數量\n",
    "12. 文章長度 \n",
    "13. 文章字母數 \n",
    "14. 文章句子數 \n",
    "15. 文章平均字長 \n",
    "16. 文章平均句長 \n",
    "17. 文章圖片數 \n",
    "18. Bonus數量 \n",
    "19. 文章斜體字數量 \n",
    "20. 文章gallery數量 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(\"C:\\\\Users\\\\stat_pc\\\\Desktop\\\\深度學習\\\\Competition 01\\\\train.csv\") \n",
    "df_raw_test = pd.read_csv(\"C:\\\\Users\\\\stat_pc\\\\Desktop\\\\深度學習\\\\Competition 01\\\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "parser = English()\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "stop_word = nltk.corpus.stopwords.words('english')\n",
    "my_stop_word = [\"also\",\"would\",\"year\",\"time\",\"take\",\"could\",\"make\",\"show\",\n",
    "                \"know\",\"like\",\"associate\",\"even\",\"much\",\"many\",\"much\",\"something\",\n",
    "                \"things\",\"look\",\"said\",\"says\",\"say\",\"on\",\"in\",\"it\",\"our\",\n",
    "                \"an\",\"and\",\"are\",\"at\",\"for\",\"of\",\"he\"]\n",
    "for i in my_stop_word:\n",
    "    stop_word.append(i)\n",
    "en_stop = set(stop_word)\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "def lemma_words(doc):\n",
    "    lm_wd = []\n",
    "    for w in analyzer(doc):\n",
    "        tag = nltk.pos_tag([w])\n",
    "        tag = get_wordnet_pos(tag[0][1]) or wordnet.NOUN\n",
    "        lm_wd.append(WordNetLemmatizer().lemmatize(w,pos =tag))\n",
    "    return (lm_wd)\n",
    "def get_lemma2(word):\n",
    "    tag = nltk.pos_tag([word])\n",
    "    tag = get_wordnet_pos(tag[0][1]) or wordnet.NOUN\n",
    "    return WordNetLemmatizer().lemmatize(word,pos=tag)\n",
    "def get_lemma3(word):\n",
    "    words = re.split('\\s+', word)\n",
    "    if len(words)==1:\n",
    "        return(get_lemma2(word))\n",
    "    else :\n",
    "        sp_word = lemma_words(word)\n",
    "        sp_word = \" \".join(sp_word)\n",
    "        return(sp_word)\n",
    "def tokenize(text):\n",
    "    \n",
    "    lda_tokens = []\n",
    "    if len(text)>1000000:\n",
    "        text = text[:1000000]\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('BLOG_ID')\n",
    "        else:\n",
    "            token = token.lower_\n",
    "            lda_tokens.append(token)\n",
    "    return(lda_tokens)\n",
    "def prepare_text_for_lda(text):\n",
    "    \n",
    "    tokens = tokenize(text)\n",
    "    tokens =[token for token in tokens if len(token)>3]\n",
    "    tokens =[token for token in tokens if token not in en_stop]\n",
    "    return tokens\n",
    "def transform_to_data_mat(lda_document_distribution,k):\n",
    "    outcome= []\n",
    "    for distribution in lda_document_distribution:\n",
    "        outcome_mat = np.zeros(k)\n",
    "        for i,j in distribution:\n",
    "            outcome_mat[i]=j\n",
    "        outcome.append(outcome_mat)\n",
    "    outcome = [w for w in outcome if w !=[]]\n",
    "    return(np.array(outcome))\n",
    "def text_to_token(text):\n",
    "    text_data = []\n",
    "    #ps = PorterStemmer()\n",
    "    for i in text:\n",
    "        tokens =prepare_text_for_lda(i)\n",
    "        #lemmatization\n",
    "        tokens=[get_lemma2(w) for w in tokens]\n",
    "        text_data.append(tokens)\n",
    "    return(text_data)\n",
    "def lda_modeling_outcome(token_text,k):\n",
    "    dictionary =corpora.Dictionary(token_text)\n",
    "    corpus = [dictionary.doc2bow(text) for text in token_text]\n",
    "    pickle.dump(corpus,open('corpus.pkl','wb'))\n",
    "    dictionary.save('dictionary.gensim')\n",
    "    NUM_TOPICS = k\n",
    "    ldamodel = models.LdaModel(corpus,num_topics = NUM_TOPICS,\n",
    "                                    id2word=dictionary,passes=5)\n",
    "    final_outcome = ldamodel.get_document_topics(corpus)\n",
    "    outcome = transform_to_data_mat(final_outcome,NUM_TOPICS)\n",
    "    return(outcome)\n",
    "def feature_ect(text) :\n",
    "    soup = BeautifulSoup(text,\"html.parser\")\n",
    "    # 抓取圖片 caption\n",
    "    # tag = soup.figcaption\n",
    "    # if(tag is None):\n",
    "    #     tag = \"no_content_error\"\n",
    "    # else:\n",
    "    #     if(soup.figcaption.string is None):\n",
    "    #         tag = \"no_content_error\"\n",
    "    #     else:    \n",
    "    #         tag = soup.figcaption.get_text()\n",
    "    # 抓取標題\n",
    "    title = soup.h1.get_text().lower()\n",
    "    n = len(title)\n",
    "    title = re.sub(\";|'.'|#|,|’s|'s|'\",\"\",title,n)\n",
    "    title_len = len(re.split(\"\\s+\",title))\n",
    "    # 抓取時間字串\n",
    "    d = soup.find(\"time\") # time \n",
    "    d_string = str(d.get_text())\n",
    "    # 抓取姓名\n",
    "    name = soup.span\n",
    "    # 檢驗作者姓名是否存在\n",
    "    if soup.span is None:\n",
    "        name = soup.select('div>a')[0].get_text()\n",
    "    else :\n",
    "        name = name.get_text()\n",
    "    # 檢驗作者姓名內是否有額外字串\n",
    "    catch_name_problem = re.match(string=name,pattern = \"[\" \"|b|B][Y|y] .*?([0-9]|\\,)\")\n",
    "    if catch_name_problem is None:\n",
    "        name=name\n",
    "    else:\n",
    "        name = name[(catch_name_problem.regs[0][0]+3):(catch_name_problem.regs[0][1]-1)]\n",
    "    # 抓取底部 topic\n",
    "    topic_group = [get_lemma3(i.get_text().lower() )for i in soup.footer.find_all('a') ]\n",
    "    # topic length (還沒寫)\n",
    "    topic_group_len = len(topic_group)\n",
    "    #topic 黏成字串，方便 countvector\n",
    "    topic_group = \"_\".join(topic_group) \n",
    "\n",
    "    # 抓取 cate 並且將所有變數併入\n",
    "    # 0:reporter name,1:topic,2:cate,3:time,4:title,5:figure caption,6:topic len,7:title len\n",
    "    outcome = [name.lower(),topic_group,\n",
    "            soup.article.attrs['data-channel'].lower(),\n",
    "            d_string,title.lower(),0,topic_group_len,title_len]\n",
    "    return(outcome)\n",
    "\n",
    "def data_precess_routine(train,test):\n",
    "    # numer of training data\n",
    "    n_train = train.shape[0]\n",
    "    # 合併 train and test\n",
    "    Y_train = train['Popularity']\n",
    "    dtf = pd.concat([train[[\"Id\",\"Page content\"]],test],ignore_index=True)\n",
    "    # 產生 list 放置新變數\n",
    "    ft_list = []\n",
    "    for i in np.arange(dtf.shape[0]):\n",
    "        ft_list.append(feature_ect(dtf[\"Page content\"][i])) \n",
    "    # 將新變數的list 轉為 numpy list \n",
    "    ft_list= np.array(ft_list)\n",
    "    # 將時區做合適的轉換\n",
    "    # 切割時間點\n",
    "    year = [];month = [];day = [];hour = [];hday = [];date=[]\n",
    "    for i in range(ft_list.shape[0]):\n",
    "        time = ft_list[i,3]\n",
    "        out = re.search(string = time,pattern = \"[0-9]$\")\n",
    "        if out is not None :\n",
    "            time=datetime.datetime.strptime(time,\"%Y-%m-%d %H:%M:%S %z\").astimezone(tz=pytz.utc)\n",
    "        else :\n",
    "            if time=='':\n",
    "                time= datetime.datetime.strptime('2000-01-01 00:00:00',\"%Y-%m-%d %H:%M:%S\").astimezone(tz=pytz.utc)\n",
    "            else :\n",
    "                time=re.sub(\"UTC\",\"-0000\",time,1)\n",
    "                time=datetime.datetime.strptime(time,\"%Y-%m-%d %H:%M:%S %z\").astimezone(tz=pytz.utc)\n",
    "        \n",
    "        time1 = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        time2 = time.strftime(\"%Y-%m-%a %H:%M:%S\")\n",
    "        year.append(time2[0:4]);month.append(time2[5:7]);hour.append(time2[12:14]);date.append(time2[8:11])\n",
    "        day.append(time1[8:10]);hday.append(time1 in holidays.US())\n",
    "    # topic 轉成 matrix，更改 max_df 與 min_df\n",
    "    count_transform = CountVectorizer(tokenizer = lambda x: x.split(\"_\"), analyzer=\"word\",max_df=39470,min_df=20)\n",
    "    topic_mat = count_transform.fit_transform(ft_list[:,1])\n",
    "    # title 轉換成 matrix，更改 max_df 與 min_df\n",
    "    title_cvtr = CountVectorizer(tokenizer=word_tokenize,analyzer=lemma_words,stop_words=stop_word,max_df=39470,min_df=20)\n",
    "    title_mat =  title_cvtr.fit_transform(ft_list[:,4])\n",
    "    # figure acption 轉換成 matrix，更改 max_df 與 min_df\n",
    "    #figtag_cvtr = CountVectorizer(tokenizer=word_tokenize,analyzer=lemma_words,stop_words=stop_word,max_df=39470,min_df=20)\n",
    "    #figtag_mat =  figtag_cvtr.fit_transform(ft_list[:,5])\n",
    "    # 輸出dataframe\n",
    "    # 並在此增加連續的日，以月中為切割點，為 -1~1 之間的數字\n",
    "    ft_frame = {\n",
    "    'Id':dtf['Id'],'reporter_name':ft_list[:,0],'Cate':ft_list[:,2],'Year':year,\n",
    "    'Month':month,'Date':date,'Hour':hour,'Holiday':hday,\n",
    "    'day_conti':[ (int(i)-15)/31 for i in day ],\n",
    "    'day_conti_2':[ ((int(i)-15)/31)**2 for i in day ],\n",
    "    'Topic length':[int(i) for i in ft_list[:,6]],\n",
    "    \"title length\": [int(i) for i in ft_list[:,7]]\n",
    "    }\n",
    "    ft_frame = pd.DataFrame(ft_frame)\n",
    "    # 與 topic 合併\n",
    "    ft_frame=pd.concat([ft_frame,\n",
    "                        pd.DataFrame(topic_mat.toarray()),\n",
    "                        pd.DataFrame(title_mat.toarray())],axis=1)\n",
    "    # 轉換 one-hot\n",
    "    ft_frame=pd.get_dummies(ft_frame,columns=[\"reporter_name\",\"Cate\",\"Year\",\"Month\",\"Hour\",\"Date\",\"Holiday\"])\n",
    "    # 切出 train\n",
    "    train_frame = pd.concat([Y_train,ft_frame[:n_train]],axis=1)\n",
    "    return((train_frame,ft_frame[n_train:]))\n",
    "def text_lda(train,test,k):\n",
    "    n_train = train.shape[0]\n",
    "    data = full = pd.concat([train,test],ignore_index=True)\n",
    "    full_text = []\n",
    "    for text in data['Page content']:\n",
    "        soup = BeautifulSoup(text,\"html.parser\")\n",
    "        inner_text = soup.select(\"article>section\")[0].get_text()\n",
    "        full_text.append(inner_text)\n",
    "    token_text = text_to_token(full_text)\n",
    "    lda_outcome = lda_modeling_outcome(token_text,k=k)\n",
    "    lda_outcome  = pd.DataFrame(lda_outcome)\n",
    "    return((lda_outcome[:n_train],lda_outcome[n_train:]))\n",
    "def data_process(train,test,k):\n",
    "    train_process_part1 = data_precess_routine(train = train,test=test)\n",
    "    train_process_part2 = text_lda(train = train,test=test,k=k)\n",
    "    train_process = pd.concat([train_process_part1[0],train_process_part2[0]],axis=1)\n",
    "    test_process = pd.concat([train_process_part1[1],train_process_part2[1]],axis=1)\n",
    "    return((train_process,test_process))\n",
    "def feature(df):\n",
    "    import re\n",
    "    from bs4 import BeautifulSoup\n",
    "    def preprocessor(text):\n",
    "        # remove HTML tags\n",
    "        text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "        text = re.sub('[\\W]+', ' ', text.lower()) + ' '\n",
    "        return text\n",
    "    def tokenizer(text):\n",
    "        return re.split('\\s+', text.strip())\n",
    "\n",
    "    lengthem = [];lengthSeealso = [];lengthBonus = [];lengthGallery = [];length = []\n",
    "    lengthimg = [];lengthmashable = [];lengthIG = [];lengthtwitter = []\n",
    "    lengthsentence = [];lengthcharacter = [];average_word_length = [];average_sentence_length = []\n",
    "    for i in range(df.shape[0]):\n",
    "        soup = BeautifulSoup(df.loc[i,\"Page content\"], 'html.parser')\n",
    "        # twitter-tweet\n",
    "        tag = soup.find_all(\"blockquote\", class_=\"twitter-tweet\")\n",
    "        if(tag is None):\n",
    "            lengthtwitter.append(0)\n",
    "        else:\n",
    "            lengthtwitter.append(len(tag))\n",
    "        # instagram.com number\n",
    "        tag = soup.find_all(href=True)\n",
    "        IGcontent = []\n",
    "        for i in range(len(tag)):\n",
    "            if(\"instagram.com\" in tag[i][\"href\"]):\n",
    "                IGcontent.append(i)\n",
    "        if(IGcontent is None):\n",
    "            lengthIG.append(0)\n",
    "        else:\n",
    "            lengthIG.append(len(IGcontent))\n",
    "        # mashable.com number\n",
    "        tag = soup.find_all(href=True)\n",
    "        mashablecontent = []\n",
    "        for i in range(len(tag)):\n",
    "            if(\"mashable.com\" in tag[i][\"href\"]):\n",
    "                mashablecontent.append(i)\n",
    "        if(mashablecontent is None):\n",
    "            lengthmashable.append(0)\n",
    "        else:\n",
    "            lengthmashable.append(len(mashablecontent))\n",
    "        # em  \n",
    "        tag = soup.find_all(\"em\")\n",
    "        if(tag is None):\n",
    "            lengthem.append(0)\n",
    "        else:\n",
    "            lengthem.append(len(tag))\n",
    "        # bonus    \n",
    "        tag = soup.find(\"div\", class_=\"bonus-content\")\n",
    "        if(tag is None):\n",
    "            lengthBonus.append(0)\n",
    "        else:\n",
    "            lengthBonus.append(len(tag))   \n",
    "        #  gallery\n",
    "        tag = soup.find(\"section\", class_=\"gallery\")\n",
    "        if(tag is None):\n",
    "            lengthGallery.append(0)\n",
    "        else:\n",
    "            lengthGallery.append(len(tag(\"li\", class_=\"slide\")))\n",
    "        # See also\n",
    "        tag = soup.find(\"article\").get_text().lower()\n",
    "        length_of_see_also = len(re.findall(pattern = \"see also:\",string=tag))\n",
    "        lengthSeealso.append(length_of_see_also)\n",
    "        # 文章字母數\n",
    "        length_of_character = sum(len(word) for word in tag.split(\" \"))\n",
    "        lengthcharacter.append(length_of_character)\n",
    "        # 文章長度\n",
    "        length_of_article = len(tag.split(\" \"))\n",
    "        length.append(length_of_article)\n",
    "        # 文章句子數\n",
    "        length_of_sentence = len(tag.split(\".\"))\n",
    "        lengthsentence.append(length_of_sentence)\n",
    "        # 文章平均字長\n",
    "        avg_w = length_of_character/length_of_article\n",
    "        average_word_length.append(avg_w)\n",
    "        # 文章平均句長\n",
    "        avg_s = length_of_article/length_of_sentence\n",
    "        average_sentence_length.append(avg_s)\n",
    "        # img\n",
    "        tag = soup.find_all(\"img\")\n",
    "        if(tag is None):\n",
    "            lengthimg.append(0)\n",
    "        else:\n",
    "            lengthimg.append(len(soup.find_all(\"img\")))\n",
    "    \n",
    "    # twitter quote number    \n",
    "    lengthtwitter = pd.DataFrame(lengthtwitter)\n",
    "    lengthtwitter.columns = [\"twitter quote number\"]\n",
    "    # Mashable number\n",
    "    lengthmashable = pd.DataFrame(lengthmashable)\n",
    "    lengthmashable.columns = [\"Mashable number\"]\n",
    "    # IG number\n",
    "    lengthIG = pd.DataFrame(lengthIG)\n",
    "    lengthIG.columns = [\"IG number\"]\n",
    "    # em\n",
    "    lengthem = pd.DataFrame(lengthem)\n",
    "    lengthem.columns = [\"em length\"]\n",
    "    # seealso\n",
    "    lengthSeealso = pd.DataFrame(lengthSeealso)\n",
    "    lengthSeealso.columns = [\"Seealso length\"]\n",
    "    #bonus\n",
    "    lengthBonus = pd.DataFrame(lengthBonus)\n",
    "    lengthBonus.columns = [\"bonus length\"]\n",
    "    #gallery\n",
    "    lengthGallery = pd.DataFrame(lengthGallery)\n",
    "    lengthGallery.columns = [\"Gallery length\"]\n",
    "    # 文章 length\n",
    "    length = pd.DataFrame(length)\n",
    "    length.columns = [\"article length\"]\n",
    "    # sentence length\n",
    "    lengthsentence = pd.DataFrame(lengthsentence)\n",
    "    lengthsentence.columns = [\"length of sentence\"]\n",
    "    # character length \n",
    "    lengthcharacter = pd.DataFrame(lengthcharacter)\n",
    "    lengthcharacter.columns = [\"Character length\"]\n",
    "    # average word length \n",
    "    average_word_length=pd.DataFrame(average_word_length)\n",
    "    average_word_length.columns = [\"average word length\"]\n",
    "    #  Average sentence length \n",
    "    average_sentence_length =pd.DataFrame(average_sentence_length)\n",
    "    average_sentence_length.columns = [\"average sentence length\"]\n",
    "    #img\n",
    "    lengthimg = pd.DataFrame(lengthimg)\n",
    "    lengthimg.columns = [\"Img length\"]\n",
    "\n",
    "    data = pd.concat([lengthem,lengthSeealso,lengthBonus,lengthGallery,\n",
    "                    length,lengthimg,lengthmashable,lengthIG,lengthtwitter,\n",
    "                    lengthsentence,lengthcharacter,average_word_length,average_sentence_length],\n",
    "                    axis=1)\n",
    "    return(data)\n",
    "#%%\n",
    "def feature_ect_title(text) :\n",
    "    soup = BeautifulSoup(text,\"html.parser\")\n",
    "    # 抓取標題\n",
    "    title = soup.h1.get_text().lower()\n",
    "    n = len(title)\n",
    "    title = re.sub(\";|'.'|#|,|’s|'s|'\",\"\",title,n)\n",
    "    outcome = title.lower()\n",
    "    return(outcome)\n",
    "\n",
    "full_data = pd.concat([df_raw,df_raw_test],axis = 0,ignore_index=True)\n",
    "ls_title = []\n",
    "for i in range(full_data.shape[0]):\n",
    "    print(i)\n",
    "    ls_title.append( feature_ect_title(full_data[\"Page content\"][i]))\n",
    "ls_title = np.array(ls_title)\n",
    "title_cvtr = CountVectorizer(tokenizer=word_tokenize,analyzer=lemma_words)\n",
    "title_mat =  title_cvtr.fit_transform(ls_title)\n",
    "title_mat_array = title_mat.toarray()\n",
    "\n",
    "#%%\n",
    "def feature_ect_fig_content(text):\n",
    "    soup = BeautifulSoup(text,\"html.parser\")\n",
    "    # 抓取圖片 caption\n",
    "    tag = soup.figcaption\n",
    "    if(tag is None):\n",
    "        tag = \"no_content_error\"\n",
    "    else:\n",
    "        if(soup.figcaption.string is None):\n",
    "            tag = \"no_content_error\"\n",
    "        else:    \n",
    "            tag = soup.figcaption.get_text()\n",
    "    return tag\n",
    "ls_pg_content = []\n",
    "for i in range(full_data.shape[0]):\n",
    "    print(i)\n",
    "    ls_pg_content.append(feature_ect_fig_content(full_data['Page content'][i]) )\n",
    "ls_pg_content = np.array(ls_pg_content)\n",
    "figtag_cvtr = CountVectorizer(tokenizer=word_tokenize,analyzer=lemma_words,stop_words=stop_word)\n",
    "figtag_mat =  figtag_cvtr.fit_transform(ls_pg_content)\n",
    "figtag_cvtr.get_feature_names()\n",
    "\n",
    "#%%\n",
    "def feature_ect_footer(text) :\n",
    "    soup = BeautifulSoup(text,\"html.parser\")\n",
    "    # 抓取底部 topic\n",
    "    topic_group = [get_lemma3(i.get_text().lower() )for i in soup.footer.find_all('a') ]\n",
    "    #topic 黏成字串，方便 countvector\n",
    "    topic_group = \"_\".join(topic_group) \n",
    "    outcome = topic_group\n",
    "    return(outcome)\n",
    "ls_footer = []\n",
    "for i in range(full_data.shape[0]):\n",
    "    print(i)\n",
    "    ls_footer.append(feature_ect_footer(full_data['Page content'][i]) )\n",
    "ls_footer = np.array(ls_footer)\n",
    "count_transform = CountVectorizer(tokenizer = lambda x: x.split(\"_\"), analyzer=\"word\")\n",
    "topic_mat = count_transform.fit_transform(ls_footer)\n",
    "count_transform.get_feature_names()\n",
    "\n",
    "#%%\n",
    "def week_day_to_int(week_day):\n",
    "    if week_day==\"Mon\":\n",
    "        return(1)\n",
    "    elif week_day==\"Tue\":\n",
    "        return(2)\n",
    "    elif week_day==\"Wed\":\n",
    "        return(3)\n",
    "    elif week_day==\"Thu\":\n",
    "        return(4)\n",
    "    elif week_day==\"Fri\":\n",
    "        return(5)\n",
    "    elif week_day==\"Sat\":\n",
    "        return(6)\n",
    "    elif week_day==\"Sun\":\n",
    "        return(7)\n",
    "        \n",
    "def feature_ect_other(text):\n",
    "    soup = BeautifulSoup(text,\"html.parser\")\n",
    "    # 抓取標題\n",
    "    title = soup.h1.get_text().lower()\n",
    "    n = len(title)\n",
    "    title = re.sub(\";|'.'|#|,|’s|'s|'\",\"\",title,n)\n",
    "    title_len = len(re.split(\"\\s+\",title))\n",
    "    # 抓取時間字串\n",
    "    d = soup.find(\"time\") # time \n",
    "    d_string = str(d.get_text())\n",
    "    # 抓取姓名\n",
    "    name = soup.span\n",
    "    # 檢驗作者姓名是否存在\n",
    "    if soup.span is None:\n",
    "        name = soup.select('div>a')[0].get_text()\n",
    "    else :\n",
    "        name = name.get_text()\n",
    "    # 檢驗作者姓名內是否有額外字串\n",
    "    catch_name_problem = re.match(string=name,pattern = \"[\" \"|b|B][Y|y] .*?([0-9]|\\,)\")\n",
    "    if catch_name_problem is None:\n",
    "        name=name\n",
    "    else:\n",
    "        name = name[(catch_name_problem.regs[0][0]+3):(catch_name_problem.regs[0][1]-1)]\n",
    "    # 抓取底部 topic\n",
    "    topic_group = [get_lemma3(i.get_text().lower() )for i in soup.footer.find_all('a') ]\n",
    "    # topic length \n",
    "    topic_group_len = len(topic_group)\n",
    "    #topic 黏成字串，方便 countvector\n",
    "    #topic_group = \"_\".join(topic_group) \n",
    "\n",
    "    # 抓取 cate 並且將所有變數併入\n",
    "    # 0:reporter name,1:cate,2:time,3:topic len,4:title len\n",
    "    outcome = [name.lower(),soup.article.attrs['data-channel'].lower(),\n",
    "            d_string,topic_group_len,title_len]\n",
    "    return(outcome)\n",
    "\n",
    "def data_precess_routine_other(train,test):\n",
    "    # numer of training data\n",
    "    n_train = train.shape[0]\n",
    "    # 合併 train and test\n",
    "    Y_train = train['Popularity']\n",
    "    dtf = pd.concat([train[[\"Id\",\"Page content\"]],test],ignore_index=True)\n",
    "    # 產生 list 放置新變數\n",
    "    ft_list = []\n",
    "    for i in np.arange(dtf.shape[0]):\n",
    "        ft_list.append(feature_ect_other(dtf[\"Page content\"][i])) \n",
    "    # 將新變數的list 轉為 numpy list \n",
    "    ft_list= np.array(ft_list)\n",
    "    # 將時區做合適的轉換\n",
    "    # 切割時間點\n",
    "    year = [];month = [];day = [];hour = [];hday = [];date=[]\n",
    "    for i in range(ft_list.shape[0]):\n",
    "        time = ft_list[i,2]\n",
    "        out = re.search(string = time,pattern = \"[0-9]$\")\n",
    "        if out is not None :\n",
    "            time=datetime.datetime.strptime(time,\"%Y-%m-%d %H:%M:%S %z\").astimezone(tz=pytz.utc)\n",
    "        else :\n",
    "            if time=='':\n",
    "                time= datetime.datetime.strptime('2000-01-01 00:00:00',\"%Y-%m-%d %H:%M:%S\").astimezone(tz=pytz.utc)\n",
    "            else :\n",
    "                time=re.sub(\"UTC\",\"-0000\",time,1)\n",
    "                time=datetime.datetime.strptime(time,\"%Y-%m-%d %H:%M:%S %z\").astimezone(tz=pytz.utc)\n",
    "        \n",
    "        time1 = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        time2 = time.strftime(\"%Y-%m-%a %H:%M:%S\")\n",
    "        year.append(time2[0:4]);month.append(time2[5:7]);hour.append(time2[12:14]);date.append(time2[8:11])\n",
    "        day.append(time1[8:10]);hday.append(time1 in holidays.US())\n",
    "    # 輸出dataframe\n",
    "    # 並在此增加連續的日，以月中為切割點，為 -1~1 之間的數字\n",
    "\n",
    "    ft_frame = {\n",
    "    'Id':dtf['Id'],'reporter_name':ft_list[:,0],\n",
    "    'Cate':ft_list[:,1],\n",
    "    'Year':[int(i)-2013 for i in year],\n",
    "    'Month':[ (int(i)-6)/12 for i in month],\n",
    "    'Month_2':[ ((int(i)-6)/12)**2 for i in month],\n",
    "    'Date':[(week_day_to_int(i)-3.5)/7 for i in date],\n",
    "    'Date_2':[((week_day_to_int(i)-3.5)/7)**2 for i in date],\n",
    "    'Hour':[(int(i)-12)/24 for i in hour],\n",
    "    'Hour_2':[ ((int(i)-12)/24)**2  for i in hour],\n",
    "    'Holiday':[int(i) for i in hday],\n",
    "    'day_conti':[ (int(i)-15)/31 for i in day ],\n",
    "    'day_conti_2':[ ((int(i)-15)/31)**2 for i in day ],\n",
    "    'Topic length':[int(i) for i in ft_list[:,3]],\n",
    "    \"title length\": [int(i) for i in ft_list[:,4]]\n",
    "    }\n",
    "    ft_frame = pd.DataFrame(ft_frame)\n",
    "    # 轉換 one-hot\n",
    "    ft_frame=pd.get_dummies(ft_frame,columns=[\"reporter_name\",\"Cate\"])\n",
    "    # 切出 train\n",
    "    train_frame = pd.concat([Y_train,ft_frame[:n_train]],axis=1)\n",
    "    return((train_frame,ft_frame[n_train:]))\n",
    "#%%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、挑選feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建完feature後再根據feature(1)、(5)、(6)、(7)、(8)~(20)及其他分成6組，分別觀察各組XGboost tree的importances，以importances當依據挑選變數。 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,KFold\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "import os\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata=pd.read_csv('C:/Users/stat-pc/Desktop/10901/deep_learning/contest1/data/test.csv/test.csv')\n",
    "df=pd.read_csv('C:/Users/stat-pc/Desktop/10901/deep_learning/contest1/data/train.csv/train.csv')\n",
    "testID=testdata[\"Id\"]\n",
    "y=df[\"Popularity\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #%%\n",
    "# a=pd.read_csv('title_colnames.csv').iloc[np.argsort(model.feature_importances_)[::-1],:]\n",
    "# a['scor']=model.feature_importances_[np.argsort(model.feature_importances_)[::-1]]\n",
    "\n",
    "# # %%\n",
    "# c=pd.read_csv('topic_colnames.csv').iloc[np.argsort(model.feature_importances_)[::-1],:]\n",
    "# c['scor']=model.feature_importances_[np.argsort(model.feature_importances_)[::-1]]\n",
    "# #%%\n",
    "# d=pd.read_csv('other_colnames.csv').iloc[6:1634,:].iloc[np.argsort(model.feature_importances_)[::-1],:]\n",
    "# d['scor']=model.feature_importances_[np.argsort(model.feature_importances_)[::-1]]\n",
    "\n",
    "# #%%\n",
    "# a.to_csv(\"a.csv\")\n",
    "# c.to_csv(\"c.csv\")\n",
    "# d.to_csv(\"d.csv\")\n",
    "\n",
    "a=pd.read_csv(\"a.csv\")\n",
    "d=pd.read_csv(\"d.csv\")\n",
    "c=pd.read_csv(\"c.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=scipy.sparse.load_npz(\"title_train_csr.npz\")\n",
    "test=scipy.sparse.load_npz(\"title_test_csr.npz\")\n",
    "ae=df[:,a['Unnamed: 0'][:15]]\n",
    "aet=test[:,a['Unnamed: 0'][:15]]\n",
    "\n",
    "df=scipy.sparse.load_npz(\"topic_train_csr.npz\")\n",
    "test=scipy.sparse.load_npz(\"topic_test_csr.npz\")\n",
    "ce=df[:,c['Unnamed: 0'][:15]]\n",
    "cet=test[:,c['Unnamed: 0'][:15]]\n",
    "\n",
    "df=scipy.sparse.load_npz(\"csr_train_other.npz\")[:,6:1634]\n",
    "test=scipy.sparse.load_npz(\"csr_test_other.npz\")[:,6:1634]\n",
    "de=df[:,d['Unnamed: 0'][:20]-6]\n",
    "det=test[:,d['Unnamed: 0'][:20]-6]\n",
    "\n",
    "e=csr_matrix(scipy.sparse.hstack([scipy.sparse.load_npz(\"csr_train_othere.npz\")[:,:12],scipy.sparse.load_npz(\"csr_train_other.npz\")[:,1640:]]))\n",
    "et=csr_matrix(scipy.sparse.hstack([scipy.sparse.load_npz(\"csr_test_othere.npz\")[:,:12],scipy.sparse.load_npz(\"csr_test_other.npz\")[:,1640:]]))\n",
    "\n",
    "hsin=csr_matrix(pd.read_csv(\"dominic_train.csv\",index_col=False).values)\n",
    "hsint=csr_matrix(pd.read_csv(\"dominic_test.csv\",index_col=False).values)\n",
    "\n",
    "lda=csr_matrix(pd.read_csv(\"LDA_train.csv\",index_col=False).iloc[:,1:])\n",
    "ldat=csr_matrix(pd.read_csv(\"LDA_test.csv\",index_col=False).iloc[:,1:])\n",
    "\n",
    "\n",
    "df=scipy.sparse.hstack([ae,ce,de,e,hsin,lda])\n",
    "df=csr_matrix(df)\n",
    "\n",
    "test=scipy.sparse.hstack([aet,cet,det,et,hsint,ldat])\n",
    "test=csr_matrix(test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " parameters = {'boosting_type': ['gbdt'],\n",
    " 'colsample_bytree': [0.4],\n",
    " 'learning_rate':[0.007],\n",
    " 'max_depth':[-1],\n",
    " 'n_estimators': [1000],\n",
    " 'num_leaves': [46],\n",
    " 'objective': ['binary'],\n",
    " 'reg_alpha': [0.002],\n",
    " 'reg_lambda': [0.5],\n",
    " 'subsample': [1],\n",
    " 'min_child_samples':[5],\n",
    " 'min_child_weight':[1e-3],\n",
    " 'importance_type':['gain']\n",
    " }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "0.6013496659921974\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    7.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    7.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'colsample_bytree': 0.4,\n",
       " 'importance_type': 'gain',\n",
       " 'learning_rate': 0.007,\n",
       " 'max_depth': -1,\n",
       " 'min_child_samples': 5,\n",
       " 'min_child_weight': 0.001,\n",
       " 'n_estimators': 1000,\n",
       " 'num_leaves': 46,\n",
       " 'objective': 'binary',\n",
       " 'reg_alpha': 0.002,\n",
       " 'reg_lambda': 0.5,\n",
       " 'subsample': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf =  GridSearchCV(lgb.LGBMClassifier(),\n",
    "                                parameters,\n",
    "                                cv = 3,\n",
    "                                scoring = 'roc_auc',\n",
    "                                n_jobs = -1,\n",
    "                                verbose = 10,\n",
    "                                refit=False)\n",
    "clf.fit(df,y)\n",
    "\n",
    "print(clf.best_score_)\n",
    "clf.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lgb.LGBMClassifier(**clf.best_params_)\n",
    "\n",
    "model.fit(df,y)\n",
    "\n",
    "print(sum(model.feature_importances_==0))\n",
    "sum(model.feature_importances_!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[:,model.feature_importances_!=0]\n",
    "test=test[:,model.feature_importances_!=0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " parameters = {'boosting_type': ['gbdt'],\n",
    " 'colsample_bytree': [0.4],\n",
    " 'learning_rate':[0.0065],\n",
    " 'max_depth':[-1],\n",
    " 'n_estimators': [1000],\n",
    " 'num_leaves': [46],\n",
    " 'objective': ['binary'],\n",
    " 'reg_alpha': [0.002],\n",
    " 'reg_lambda': [0.6],\n",
    " 'subsample': [1],\n",
    " 'min_child_samples':[5],\n",
    " 'min_child_weight':[1e-3],\n",
    " 'importance_type':['gain']\n",
    " }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "0.6015572108107315\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    7.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    7.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'colsample_bytree': 0.4,\n",
       " 'importance_type': 'gain',\n",
       " 'learning_rate': 0.0065,\n",
       " 'max_depth': -1,\n",
       " 'min_child_samples': 5,\n",
       " 'min_child_weight': 0.001,\n",
       " 'n_estimators': 1000,\n",
       " 'num_leaves': 46,\n",
       " 'objective': 'binary',\n",
       " 'reg_alpha': 0.002,\n",
       " 'reg_lambda': 0.6,\n",
       " 'subsample': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf =  GridSearchCV(lgb.LGBMClassifier(),\n",
    "                                parameters,\n",
    "                                cv = 3,\n",
    "                                scoring = 'roc_auc',\n",
    "                                n_jobs = -1,\n",
    "                                verbose = 10,\n",
    "                                refit=False)\n",
    "clf.fit(df,y)\n",
    "\n",
    "print(clf.best_score_)\n",
    "clf.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lgb.LGBMClassifier(**clf.best_params_)\n",
    "\n",
    "model.fit(df,y)\n",
    "\n",
    "print(sum(model.feature_importances_==0))\n",
    "sum(model.feature_importances_!=0)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = model.predict_proba(test)[:,1]\n",
    "pred_csv= np.zeros((testdata.shape[0],2))\n",
    "pred_csv = pd.DataFrame(pred_csv)\n",
    "pred_csv.columns = ['Id','Popularity']\n",
    "pred_csv['Id'] = testID\n",
    "pred_csv['Popularity'] = ans\n",
    "pd.DataFrame(pred_csv).to_csv('y_pred_chyjj1.csv',index=False,header=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、建構模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 挑選完變數後，我們再使用LightGBM模型，首先調整reg_lambda參數，觀察feature importances的Solution Path去挑選變數，再針對不同reg_lambda挑選到的變數重新去配適LightGBM模型，並調整各式參數，最後選擇cross validation score最高的模型，即為我們本次Kaggle上private score最高的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#%%\n",
    "df_x = pd.read_csv(\"C:\\\\Users\\\\linre\\\\Desktop\\\\DL_HW\\\\DL_competition_1\\\\train.csv\")\n",
    "df_raw_test = pd.read_csv(\"C:\\\\Users\\\\linre\\\\Desktop\\\\DL_HW\\\\DL_competition_1\\\\test.csv\")\n",
    "train_y = df_x[\"Popularity\"].values\n",
    "sp_train_x = sp.sparse.load_npz(\"C:\\\\Users\\\\linre\\\\Desktop\\\\DL_HW\\\\DL_competition_1\\\\new_data\\\\train_15_20-15.npz\")\n",
    "sp_test_x = sp.sparse.load_npz(\"C:\\\\Users\\\\linre\\\\Desktop\\\\DL_HW\\\\DL_competition_1\\\\new_data\\\\test_15_20-15.npz\")\n",
    "\n",
    "parameters = {\n",
    " 'colsample_bytree': [0.3],#0.2~0.5\n",
    " 'learning_rate':[0.006],#0.0045~0.0075\n",
    " 'n_estimators': [1000],\n",
    " 'num_leaves': [46],#43~55\n",
    " 'objective': ['binary'],\n",
    " 'reg_alpha': [0.001],#0.0008~0.1\n",
    " 'reg_lambda': [0.3],#0.1~0.5\n",
    " 'min_child_samples':[5],#4~10\n",
    " 'min_child_weight':[1e-3],#1e-(2,3,4,5)\n",
    " 'importance_type':['gain']\n",
    "}\n",
    "\n",
    "clf =  GridSearchCV(lgb.LGBMClassifier(),\n",
    "                                parameters,\n",
    "                                cv = 3,\n",
    "                                scoring = 'roc_auc',\n",
    "                                n_jobs = 3,\n",
    "                                verbose = 10,\n",
    "                                refit=False)\n",
    "\n",
    "clf.fit(sp_train_x,train_y)\n",
    "\n",
    "print(clf.best_score_)\n",
    "\n",
    "model = lgb.LGBMClassifier(**clf.best_params_)\n",
    "model.fit(df,y)\n",
    "model.feature_importances_\n",
    "\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "num_of_zero = []\n",
    "auc = []\n",
    "importance = [] \n",
    "\n",
    "for acd_lambda in np.arange(8,21,1):\n",
    "    parameters = {\n",
    "    'colsample_bytree': [0.3,0.5],#0.2~0.5\n",
    "    'learning_rate':[0.006],#0.0045~0.0075\n",
    "    'n_estimators': [1000],\n",
    "    'num_leaves': [43,48,53],#43~55\n",
    "    'objective': ['binary'],\n",
    "    'reg_alpha': [0.001],#0.0008~0.1\n",
    "    'reg_lambda': [acd_lambda],#0.1~0.5\n",
    "    'min_child_samples':[4,8],#4~10\n",
    "    'min_child_weight':[1e-5,1e-3,1e-4],#1e-(2,3,4,5)\n",
    "    'importance_type':['gain']\n",
    "    }\n",
    "    clf =  GridSearchCV(lgb.LGBMClassifier(),\n",
    "                                parameters,\n",
    "                                cv = 3,\n",
    "                                scoring = 'roc_auc',\n",
    "                                n_jobs = 3,\n",
    "                                verbose = 0,\n",
    "                                refit=False)\n",
    "    clf.fit(sp_train_x,train_y)\n",
    "    #AUC\n",
    "    auc.append(clf.best_score_)\n",
    "    model = lgb.LGBMClassifier(**clf.best_params_)\n",
    "    model.fit(sp_train_x,train_y)\n",
    "    # importance \n",
    "    importance.append(model.feature_importances_)\n",
    "    # num of zero \n",
    "    num_of_zero.append(sum(model.feature_importances_==0))\n",
    "    print(\"done\")\n",
    "\n",
    "\n",
    "outcome_frame  = pd.concat([pd.DataFrame(num_of_zero),pd.DataFrame(auc),pd.DataFrame(importance)],axis = 1)\n",
    "outcome_frame.to_csv(\"C:\\\\Users\\\\linre\\\\Desktop\\\\DL_HW\\\\DL_competition_1\\\\variable selection\\\\selection_Frame.csv\")\n",
    "# %%\n",
    "# 16 zero \n",
    "\n",
    "zero_pos = [i for i in np.where(importance[0]==0)[0]]\n",
    "\n",
    "train_x_16 =  pd.DataFrame(sp_train_x.toarray()).drop(zero_pos,axis=1)\n",
    "test_x_16 = pd.DataFrame(sp_test_x.toarray()).drop(zero_pos,axis=1)\n",
    "sp_train_x_16 = csr_matrix(train_x_16)\n",
    "sp_test_x_16 = csr_matrix(test_x_16)\n",
    "#%%   cv 0.6017177514039636\n",
    "parameters = {\n",
    "    'colsample_bytree': [0.4],#0.2~0.5\n",
    "    'learning_rate':[0.004],#0.0045~0.0075\n",
    "    'n_estimators': [1000],\n",
    "    'num_leaves': [55],#43~55\n",
    "    'objective': ['binary'],\n",
    "    'reg_alpha': [0.0008],#0.0008~0.1\n",
    "    'reg_lambda': [0.25],#0.1~0.5\n",
    "    'min_child_samples':[4,6],#4~10\n",
    "    'min_child_weight':[1e-5],#1e-(2,3,4,5)\n",
    "    'importance_type':['gain']\n",
    "    }\n",
    "clf =  GridSearchCV(lgb.LGBMClassifier(),\n",
    "                            parameters,\n",
    "                            cv = 3,\n",
    "                            scoring = 'roc_auc',\n",
    "                            n_jobs = 3,\n",
    "                            verbose = 10,\n",
    "                            refit=False)\n",
    "clf.fit(sp_train_x_16,train_y)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n",
    "model = lgb.LGBMClassifier(**clf.best_params_)\n",
    "model.fit(sp_train_x_16,train_y)\n",
    "model.feature_importances_\n",
    "# %%\n",
    "# 17 zero 01\n",
    "zero_pos_17_1 = [i for i in np.where(importance[1]==0)[0]]\n",
    "\n",
    "train_x_17_1 =  pd.DataFrame(sp_train_x.toarray()).drop(zero_pos_17_1,axis=1)\n",
    "test_x_17_1 = pd.DataFrame(sp_test_x.toarray()).drop(zero_pos_17_1,axis=1)\n",
    "sp_train_x_17_1 = csr_matrix(train_x_17_1)\n",
    "sp_test_x_17_1 = csr_matrix(test_x_17_1)\n",
    "#%%\n",
    "#%%   cv 0.6022852284061436\n",
    "parameters = {\n",
    "    'colsample_bytree': [0.5],#0.2~0.5\n",
    "    'learning_rate':[0.004],#0.0045~0.0075\n",
    "    'n_estimators': [1000],\n",
    "    'num_leaves': [57],#43~55\n",
    "    'objective': ['binary'],\n",
    "    'reg_alpha': [0.001],#0.0008~0.1\n",
    "    'reg_lambda': [0.45],#0.1~0.5\n",
    "    'min_child_samples':[3],#4~10\n",
    "    'min_child_weight':[1e-5],#1e-(2,3,4,5)\n",
    "    'importance_type':['gain']\n",
    "    }\n",
    "clf =  GridSearchCV(lgb.LGBMClassifier(),\n",
    "                            parameters,\n",
    "                            cv = 3,\n",
    "                            scoring = 'roc_auc',\n",
    "                            n_jobs = 3,\n",
    "                            verbose = 10,\n",
    "                            refit=False)\n",
    "clf.fit(train_x_17_1,train_y)\n",
    "clf.best_params_\n",
    "print(clf.best_score_)\n",
    "model = lgb.LGBMClassifier(**clf.best_params_)\n",
    "model.fit(train_x_17_1,train_y)\n",
    "test_pred_outcome=model.predict_proba(sp_test_x_17_1)[:,1]\n",
    "# %%\n",
    "# 17 zero 02\n",
    "zero_pos_17_2 = [i for i in np.where(importance[2]==0)[0]]\n",
    "\n",
    "train_x_17_2 =  pd.DataFrame(sp_train_x.toarray()).drop(zero_pos_17_2,axis=1)\n",
    "test_x_17_2 = pd.DataFrame(sp_test_x.toarray()).drop(zero_pos_17_2,axis=1)\n",
    "sp_train_x_17_2 = csr_matrix(train_x_17_2)\n",
    "sp_test_x_17_2 = csr_matrix(test_x_17_2)\n",
    "# %% 0.6023221804589858\n",
    "parameters = {\n",
    "    'colsample_bytree': [0.5],#0.2~0.5\n",
    "    'learning_rate':[0.004],#0.0045~0.0075\n",
    "    'n_estimators': [800],\n",
    "    'num_leaves': [57],#43~55\n",
    "    'objective': ['binary'],\n",
    "    'reg_alpha': [0.001],#0.0008~0.1\n",
    "    'reg_lambda': [0.35],#0.1~0.5\n",
    "    'min_child_samples':[3],#4~10\n",
    "    'min_child_weight':[1e-5],#1e-(2,3,4,5)\n",
    "    'importance_type':['gain']\n",
    "    }\n",
    "clf =  GridSearchCV(lgb.LGBMClassifier(),\n",
    "                            parameters,\n",
    "                            cv = 3,\n",
    "                            scoring = 'roc_auc',\n",
    "                            n_jobs = 3,\n",
    "                            verbose = 10,\n",
    "                            refit=False)\n",
    "clf.fit(train_x_17_2,train_y)\n",
    "clf.best_params_\n",
    "print(clf.best_score_)\n",
    "model = lgb.LGBMClassifier(**clf.best_params_)\n",
    "model.fit(train_x_17_2,train_y)\n",
    "model.feature_importances_\n",
    "\n",
    "# %% 21 zero\n",
    "zero_pos_21 = [i for i in np.where(importance[3]==0)[0]]\n",
    "\n",
    "train_x_21 =  pd.DataFrame(sp_train_x.toarray()).drop(zero_pos_21,axis=1)\n",
    "test_x_21 = pd.DataFrame(sp_test_x.toarray()).drop(zero_pos_21,axis=1)\n",
    "sp_train_x_21 = csr_matrix(train_x_21)\n",
    "sp_test_x_21 = csr_matrix(test_x_21)\n",
    "# %% 0.6027639485565163\n",
    "parameters = {\n",
    "    'colsample_bytree': [0.5],#0.2~0.5\n",
    "    'learning_rate':[0.004],#0.0045~0.0075\n",
    "    'n_estimators': [800],\n",
    "    'num_leaves': [57],#43~55\n",
    "    'objective': ['binary'],\n",
    "    'reg_alpha': [0.01],#0.0008~0.1\n",
    "    'reg_lambda': [0.045],#0.1~0.5\n",
    "    'min_child_samples':[3],#4~10\n",
    "    'min_child_weight':[1e-5],#1e-(2,3,4,5)\n",
    "    'importance_type':['gain']\n",
    "    }\n",
    "clf =  GridSearchCV(lgb.LGBMClassifier(),\n",
    "                            parameters,\n",
    "                            cv = 3,\n",
    "                            scoring = 'roc_auc',\n",
    "                            n_jobs = 3,\n",
    "                            verbose = 10,\n",
    "                            refit=False)\n",
    "clf.fit(train_x_21,train_y)\n",
    "clf.best_params_\n",
    "print(clf.best_score_)\n",
    "model = lgb.LGBMClassifier(**clf.best_params_)\n",
    "model.fit(train_x_21,train_y)\n",
    "model.feature_importances_\n",
    "test_pred_outcome=model.predict_proba(test_x_21)[:,1]\n",
    "# %% 18 zero\n",
    "zero_pos_18 = [i for i in np.where(importance[4]==0)[0]]\n",
    "\n",
    "train_x_18 =  pd.DataFrame(sp_train_x.toarray()).drop(zero_pos_18,axis=1)\n",
    "test_x_18 = pd.DataFrame(sp_test_x.toarray()).drop(zero_pos_18,axis=1)\n",
    "sp_train_x_18 = csr_matrix(train_x_18)\n",
    "sp_test_x_18 = csr_matrix(test_x_18)\n",
    "#%% 0.6020049558034137\n",
    "parameters = {\n",
    "    'colsample_bytree': [0.5],#0.2~0.5\n",
    "    'learning_rate':[0.004],#0.0045~0.0075\n",
    "    'n_estimators': [800],\n",
    "    'num_leaves': [46],#43~55\n",
    "    'objective': ['binary'],\n",
    "    'reg_alpha': [0.01],#0.0008~0.1\n",
    "    'reg_lambda': [0.3],#0.1~0.5\n",
    "    'min_child_samples':[3],#4~10\n",
    "    'min_child_weight':[1e-5],#1e-(2,3,4,5)\n",
    "    'importance_type':['gain']\n",
    "    }\n",
    "clf =  GridSearchCV(lgb.LGBMClassifier(),\n",
    "                            parameters,\n",
    "                            cv = 3,\n",
    "                            scoring = 'roc_auc',\n",
    "                            n_jobs = 3,\n",
    "                            verbose = 10,\n",
    "                            refit=False)\n",
    "clf.fit(train_x_18,train_y)\n",
    "clf.best_params_\n",
    "print(clf.best_score_)\n",
    "model = lgb.LGBMClassifier(**clf.best_params_)\n",
    "model.fit(train_x_18,train_y)\n",
    "model.feature_importances_\n",
    "\n",
    "# %%\n",
    "num_of_zero_2 = []\n",
    "auc_2 = []\n",
    "importance_2 = [] \n",
    "\n",
    "for acd_lambda in np.arange(21,24,3):\n",
    "    parameters = {\n",
    "    'colsample_bytree': [0.3,0.5],#0.2~0.5\n",
    "    'learning_rate':[0.006],#0.0045~0.0075\n",
    "    'n_estimators': [1000],\n",
    "    'num_leaves': [43,48,53],#43~55\n",
    "    'objective': ['binary'],\n",
    "    'reg_alpha': [0.001],#0.0008~0.1\n",
    "    'reg_lambda': [acd_lambda],#0.1~0.5\n",
    "    'min_child_samples':[4,8],#4~10\n",
    "    'min_child_weight':[1e-5,1e-3,1e-4],#1e-(2,3,4,5)\n",
    "    'importance_type':['gain']\n",
    "    }\n",
    "    clf_2 =  GridSearchCV(lgb.LGBMClassifier(),\n",
    "                                parameters,\n",
    "                                cv = 3,\n",
    "                                scoring = 'roc_auc',\n",
    "                                n_jobs = 3,\n",
    "                                verbose = 0,\n",
    "                                refit=False)\n",
    "    clf_2.fit(sp_train_x,train_y)\n",
    "    #AUC\n",
    "    auc_2.append(clf_2.best_score_)\n",
    "    model_2 = lgb.LGBMClassifier(**clf_2.best_params_)\n",
    "    model_2.fit(sp_train_x,train_y)\n",
    "    # importance \n",
    "    importance_2.append(model_2.feature_importances_)\n",
    "    # num of zero \n",
    "    num_of_zero_2.append(sum(model_2.feature_importances_==0))\n",
    "    print(\"done\")\n",
    "\n",
    "\n",
    "outcome_frame_2  = pd.concat([pd.DataFrame(num_of_zero_2),pd.DataFrame(auc_2),pd.DataFrame(importance_2)],axis = 1)\n",
    "\n",
    "# %%\n",
    "outcome_frame_2.to_csv(\"C:\\\\Users\\\\linre\\\\Desktop\\\\DL_HW\\\\DL_competition_1\\\\variable selection\\\\selection_Frame_2.csv\")\n",
    "#%%\n",
    "#test_pred_outcome = model.predict_proba(test_x_csr)[:,1]\n",
    "test_pred = {\n",
    "        'Id': df_raw_test['Id'],\n",
    "        'Popularity': test_pred_outcome\n",
    "    }\n",
    "outcome = pd.DataFrame(test_pred)\n",
    "pd.DataFrame.to_csv(outcome,\"C:\\\\Users\\\\linre\\\\Desktop\\\\DL_HW\\\\DL_competition_1\\\\test_pred_514_v16.csv\",index=False)\n",
    "\n",
    "\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、結論"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (一)、一開始在比賽時，是直接將文章內容做Bag-Of-Words，然而這樣做有２個缺點，第一個是多餘變數太多，對預測有不好的影響，造成上傳分數卡在0.57後就很難再上去，第二是design matrix 會變得太大，需要的記憶體空間不足及執行效率緩慢，因此最後改採自己定義feature為主，Bag-Of-Words為輔的方式處理feature。\n",
    "## (二)、本次比賽，feature選擇的重要性高於模型選擇的重要性。無論feature太多或太少，對模型的預測皆有不好的影響，因此我們花了很多時間在挑選feature，挑選完畢後，最後在建模時，基本上只要參數調整適當，皆能有好的預測表現。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3.7.9v1]",
   "language": "python",
   "name": "conda-env-python3.7.9v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
